{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"mfcc_data_sql_2.ipynb","provenance":[{"file_id":"1_1vIW02g7ZS5cW5tr5eHR95xuoYXV9mW","timestamp":1604330743587}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":false,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false}},"cells":[{"cell_type":"code","metadata":{"id":"5WosHdRWdmpw","executionInfo":{"status":"ok","timestamp":1604599006056,"user_tz":300,"elapsed":39931,"user":{"displayName":"Eunjeong Lee","photoUrl":"","userId":"02800555256841027088"}},"outputId":"a9f73345-51d1-4e7e-a2b4-3e4739ffb5fe","colab":{"base_uri":"https://localhost:8080/"}},"source":["import os\n","# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","spark_version = 'spark-3.0.1'\n","#spark_version = 'spark-3.<enter version>'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.1 kB]\n","Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:11 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Ign:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n","Get:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [405 kB]\n","Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,353 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,687 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,118 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,750 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,167 kB]\n","Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n","Get:22 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [48.9 kB]\n","Fetched 10.7 MB in 6s (1,672 kB/s)\n","Reading package lists... Done\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xISV-9eR6Yah","executionInfo":{"status":"ok","timestamp":1604599038513,"user_tz":300,"elapsed":2412,"user":{"displayName":"Eunjeong Lee","photoUrl":"","userId":"02800555256841027088"}},"outputId":"547a3eef-7bbc-4e6b-9e1f-4aa10cfe1b7e","colab":{"base_uri":"https://localhost:8080/"}},"source":["!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-11-05 17:57:16--  https://jdbc.postgresql.org/download/postgresql-42.2.9.jar\n","Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n","Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 914037 (893K) [application/java-archive]\n","Saving to: ‘postgresql-42.2.9.jar’\n","\n","postgresql-42.2.9.j 100%[===================>] 892.61K  1.05MB/s    in 0.8s    \n","\n","2020-11-05 17:57:18 (1.05 MB/s) - ‘postgresql-42.2.9.jar’ saved [914037/914037]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aEQFTWiV6g2h"},"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"CloudETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qik6uGSV9BWa"},"source":["# Import depencies for MFCCs and loading sparkfile using librosa\n","import librosa\n","import numpy as np\n","import math\n","from pyspark import SparkFiles\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1ejWrX1ckI4f"},"source":["# AWS postgresSQL setting\n","# Configure settings for RDS\n","\n","#mode = \"append\"\n","jdbc_url=\"jdbc:postgresql://urbansounddb.cwkue3lwi5mx.us-east-1.rds.amazonaws.com:5432/postgres\"\n","config = {\"user\":\"root\", \n","          \"password\": \"urbansoundDB\", \n","          \"driver\":\"org.postgresql.Driver\"}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8mRlPX3MD34"},"source":["Preprocessing audio data and build MFCCs vectors"]},{"cell_type":"code","metadata":{"id":"9yyZgvcUnaNN"},"source":["# Define constants\n","SAMPLE_RATE = 22050\n","DURATION = 4 # measured in seconds\n","SAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\n","\n","# Define parameters for librosa mfcc function\n","#n_mfcc = 13\n","n_fft = 2048 # default number\n","hop_length = 512 # default number\n","\n","# build_MFCC_vectors(fold_name):\n","#   This function, for given folder name, builds MFCC vectors from the audio files in the folder\n","#   taking average of the MFCCs values for each coefficient\n","#   fold_name: string\n","#   n_mfcc: number of MFCCs to return\n","#   return -- list of array=[ fileID, classID, MFCCs of average ] (list size = 2+n_mfcc)\n","def build_MFCC_features(fold_name, n_mfcc=13):\n","\n","  #print(\"Start constructiong mfcc for \" + fold_name)\n","  #num_samples_per_segment = int(SAMPLES_PER_TRACK / num_segments)\n","  #expected_num_mfcc_vectors_per_segment =  math.ceil(num_samples_per_segment / hop_length)\n","\n","  # Read metadata for the folder\n","  metadata_df = spark.read.jdbc(url=jdbc_url, table=f'{fold_name}_metadata', properties=config)\n","\n","  # url S3 bucket: load audiofiles from S3 bucket\n","  fold_url_path=f\"https://ejbigdatasets.s3.amazonaws.com/UrbanSound8K/audio/{fold_name}\"\n","\n","  # Iterate the metatable, get the filenames\n","  mrows = metadata_df.collect()\n","\n","  mfcc_rows = []\n","  i = 0\n","  for frow in mrows:\n","    fn = frow.slice_file_name\n","    file_url = fold_url_path + \"/\" + fn\n","\n","    # Read an audio file fn\n","    spark.sparkContext.addFile(file_url)\n","\n","    # Load the signal samples from the audio file\n","    signal, sr = librosa.load(SparkFiles.get(fn), sr=SAMPLE_RATE)\n","\n","    # Build mfcc vectors for each segment       \n","    mfcc = librosa.feature.mfcc(signal,\n","                                sr=sr,\n","                                n_mfcc=n_mfcc,\n","                                n_fft=n_fft,\n","                                hop_length=hop_length)\n","\n","    #mfcc = mfcc.T # transpose\n","    mfcc_processed = np.mean(mfcc.T, axis=0)\n","    new_row = [int(frow.id), int(frow.classid)] + mfcc_processed.tolist()\n","    mfcc_rows.append(new_row)\n","\n","    #print(\"mfcc shape\", mfcc.shape)\n","    #print(\"size(mean of mfcc)\", len(np.mean(mfcc,axis=0)))\n","    #print(\"size(mean of mfcc.T)\", len(np.mean(mfcc.T,axis=0)))\n","\n","    #i+=1\n","    #if i>5:\n","    #  break\n","  return mfcc_rows\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAtA3fc_KUfv","executionInfo":{"status":"ok","timestamp":1604615828482,"user_tz":300,"elapsed":620933,"user":{"displayName":"Eunjeong Lee","photoUrl":"","userId":"02800555256841027088"}},"outputId":"1a081181-7df7-4d9f-f8ec-18ce2b0bb378","colab":{"base_uri":"https://localhost:8080/"}},"source":["NUMFOLDS = 10\n","n_mfcc = 40\n","\n","# Define columns for MFCCs dataframe\n","columns = ['fileID', 'classID']\n","for j in range(n_mfcc):\n","  columns.append('mfcc_avg_'+str(j))\n","print(columns)\n","\n","for i in range(2,NUMFOLDS+1):\n","  fold_name = f'fold{i}'\n","\n","  print(\"Building MFCCs for \", fold_name)\n","  mfcc_vcs = build_MFCC_features(fold_name, n_mfcc=n_mfcc)\n","\n","  # Create dataframe from mfcc vectors\n","  rdd = spark.sparkContext.parallelize(mfcc_vcs)\n","  mfcc_df = spark.createDataFrame(rdd, columns)\n","\n","  # Write the dataframe to AWS RDS\n","  table_name = f'fold{i}_mfcc_avg_n_{n_mfcc}'\n","  print(\"Writing MFCCs for \", table_name)\n","  mfcc_df.write.jdbc(url=jdbc_url, table=table_name, mode='append', properties=config)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['fileID', 'classID', 'mfcc_avg_0', 'mfcc_avg_1', 'mfcc_avg_2', 'mfcc_avg_3', 'mfcc_avg_4', 'mfcc_avg_5', 'mfcc_avg_6', 'mfcc_avg_7', 'mfcc_avg_8', 'mfcc_avg_9', 'mfcc_avg_10', 'mfcc_avg_11', 'mfcc_avg_12', 'mfcc_avg_13', 'mfcc_avg_14', 'mfcc_avg_15', 'mfcc_avg_16', 'mfcc_avg_17', 'mfcc_avg_18', 'mfcc_avg_19', 'mfcc_avg_20', 'mfcc_avg_21', 'mfcc_avg_22', 'mfcc_avg_23', 'mfcc_avg_24', 'mfcc_avg_25', 'mfcc_avg_26', 'mfcc_avg_27', 'mfcc_avg_28', 'mfcc_avg_29', 'mfcc_avg_30', 'mfcc_avg_31', 'mfcc_avg_32', 'mfcc_avg_33', 'mfcc_avg_34', 'mfcc_avg_35', 'mfcc_avg_36', 'mfcc_avg_37', 'mfcc_avg_38', 'mfcc_avg_39']\n","Building MFCCs for  fold2\n","Writing MFCCs for  fold2_mfcc_avg_n_40\n","Building MFCCs for  fold3\n","Writing MFCCs for  fold3_mfcc_avg_n_40\n","Building MFCCs for  fold4\n","Writing MFCCs for  fold4_mfcc_avg_n_40\n","Building MFCCs for  fold5\n","Writing MFCCs for  fold5_mfcc_avg_n_40\n","Building MFCCs for  fold6\n","Writing MFCCs for  fold6_mfcc_avg_n_40\n","Building MFCCs for  fold7\n","Writing MFCCs for  fold7_mfcc_avg_n_40\n","Building MFCCs for  fold8\n","Writing MFCCs for  fold8_mfcc_avg_n_40\n","Building MFCCs for  fold9\n","Writing MFCCs for  fold9_mfcc_avg_n_40\n","Building MFCCs for  fold10\n","Writing MFCCs for  fold10_mfcc_avg_n_40\n"],"name":"stdout"}]}]}